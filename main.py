# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
import os
import streamlit as st

from langchain_core.messages.chat import ChatMessage
from langchain_core.output_parsers import StrOutputParser

from load_prompt import load_prompt  # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë”
from dotenv import load_dotenv  # ì €ì¥ëœ api key ë¡œë”
from langchain_ollama import ChatOllama
import time  # ì‘ì—… ìˆ˜í–‰ ì‹œ í…€ì„ ì£¼ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ì…ë ¥í•œ ë°ì´í„° DBì— ì €ì¥í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from data_loader import HWPLoader, JsonLoader  # ì…ë ¥ íŒŒì¼ ë¡œë”
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough

# í•´ì‹œê°’ ìƒì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from hashlib import md5
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore

# ---------------------------
# ì‹œì—°ìš© DummyEmbeddings ì •ì˜
# ---------------------------
class DummyEmbeddings:
    def embed_query(self, text):
        # FAISS ê²€ìƒ‰ë§Œ ì‚¬ìš©í•  ê±°ë¼ ì‹¤ì œ ì„ë² ë”©ì€ í•„ìš” ì—†ìŒ
        return [0.0] * 1536  # ê¸°ì¡´ nomic ë²¡í„° ì°¨ì› ìˆ˜ì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ì„¸ìš”

# api key ì •ë³´ ë¡œë“œ
load_dotenv()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
for folder in [".cache", ".cache/files", "./model"]:
    if not os.path.exists(folder):
        os.mkdir(folder)
os.environ["TRANSFORMERS_CACHE"] = "./model"
os.environ["HF_HOME"] = "./model"

st.title("ì†Œë§ì´ : RAG ê¸°ë°˜ ë§ì”€ ê²€ìƒ‰ ì‹œìŠ¤í…œ")

# ê²½ê³  ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­ ìƒì„±
warning_msg = st.empty()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    st.session_state["chain"] = None

# ---------------------------
# ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
# ---------------------------
embeddings = DummyEmbeddings()  # ì‹œì—°ìš©
vectorstore = FAISS.load_local(
    folder_path="./vectorstore",
    index_name="faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True
)

# ì‚¬ì´ë“œë°” ë²„íŠ¼
with st.sidebar:
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["hwp", "json"])
    clear_btn = st.button("ì´ˆê¸°í™”")

# ë©”ì‹œì§€ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        with st.chat_message(chat_message.role):
            st.markdown(chat_message.content)

# ë©”ì‹œì§€ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

# íŒŒì¼ ì—…ë¡œë“œ ì‹œ retriever ë°˜í™˜ (ì„ë² ë”© ìƒëµ)
def embed_file(file):
    file_content = file.read()
    file_hash = md5(file_content).hexdigest()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    warning_msg.warning("âš ï¸ ì‹œì—°ìš©: ìƒˆ ì„ë² ë”© ìƒì„± ìƒëµ, ê¸°ì¡´ ë²¡í„°ë¡œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤")
    time.sleep(2)
    warning_msg.empty()

    retriever = vectorstore.as_retriever()
    return retriever

# ì²´ì¸ ìƒì„±
def create_chain(retriever, model_name="EXAONE-3.5"):
    prompt = load_prompt("prompt.yaml", encoding="utf-8")
    if model_name == "EXAONE-3.5":
        llm = ChatOllama(model="EXAONE3.5-Q8:latest", temperature=0)
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# ---------------------------
# íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
# ---------------------------
if uploaded_file:
    retriever = embed_file(uploaded_file)
    chain = create_chain(retriever, model_name="EXAONE-3.5")
    st.session_state["chain"] = chain
else:
    retriever = vectorstore.as_retriever()
    chain = create_chain(retriever, model_name="EXAONE-3.5")
    st.session_state["chain"] = chain

# ì´ˆê¸°í™” ë²„íŠ¼
if clear_btn:
    st.session_state["messages"] = []
    st.session_state["chain"] = None

# ì¶œë ¥
print_messages()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
if user_input:
    docs = retriever.invoke(user_input)
    print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜:", len(docs))
    for i, doc in enumerate(docs):
        print(f"[{i}] {doc.metadata.get('file_name')} - {doc.page_content[:100]}...")

    chain = st.session_state["chain"]
    with st.chat_message("user"):
        st.markdown(user_input)

    response = chain.stream(user_input)
    with st.chat_message("assistant"):
        container = st.empty()
        ai_answer = ""
        for token in response:
            ai_answer += token
            container.markdown(ai_answer)

    add_message("user", user_input)
    add_message("assistant", ai_answer)
