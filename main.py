# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
import os
import streamlit as st

from langchain_core.messages.chat import ChatMessage
from langchain_core.output_parsers import StrOutputParser

from load_prompt import load_prompt # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë”
from dotenv import load_dotenv # ì €ì¥ëœ api key ë¡œë”
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
import time # ì‘ì—… ìˆ˜í–‰ ì‹œ í…€ì„ ì£¼ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ì…ë ¥í•œ ë°ì´í„° DBì— ì €ì¥í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from data_loader import HWPLoader, JsonLoader # ì…ë ¥ íŒŒì¼ ë¡œë”
from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_community.embeddings import OllamaEmbeddings
from embedding_api import RemoteOllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough

# í•´ì‹œê°’ ìƒì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from hashlib import md5
import json
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore

# api key ì •ë³´ ë¡œë“œ
load_dotenv()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (.í´ë”ëª…: ìˆ¨ê¹€í´ë”)
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œì „ìš© í´ë” ìƒì„±(ì„ì‹œì €ì¥ìš©)
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# ./cache/ ê²½ë¡œì— ë‹¤ìš´ë¡œë“œ ë°›ë„ë¡ ì„¤ì •
if not os.path.exists("./model"):
    os.mkdir("./model")

os.environ["TRANSFORMERS_CACHE"] = "./model"
os.environ["HF_HOME"] = "./model"

st.title("ì†Œë§ì´ : RAG ê¸°ë°˜ ë§ì”€ ê²€ìƒ‰ ì‹œìŠ¤í…œ")

# ê²½ê³  ë©”ì„¸ì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­ ìƒì„±
warning_msg = st.empty()

# ì±„íŒ…ì´ ì°¨ê³¡ì°¨ê³¡ ìŒ“ì—¬ì•¼í•¨ --> ë©”ì„¸ì§€ë“¤ì„ ê¸°ë¡í•˜ëŠ” ê¸°ëŠ¥ì´ í•„ìš”
# streamlitì€ í˜ì´ì§€ê°€ ë§¤ë²ˆ ìƒˆë¡œê³ ì¹¨ë˜ëŠ” í˜•íƒœ -> ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•´ë‘¬ì•¼í•¨! -> í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ì´ ì¼ì–´ë‚˜ë„ ê¸°ë¡ì´ ë‚¨ì•„ìˆì–´ì•¼í•¨
# session_state: ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„(ì„¸ì…˜ ìƒíƒœë¥¼ ì €ì¥)
if "messages" not in st.session_state:  # ì²˜ìŒ í•œë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
    st.session_state["messages"] = []

# ì²˜ìŒ í•œë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ : ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì§€ ì•Šì•˜ì„ ë•Œ
if "chain" not in st.session_state:
    st.session_state["chain"] = None

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
# EMBEDDING_MODEL = "nomic-embed-text"
# embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
embeddings = RemoteOllamaEmbeddings("https://your-ollama-server.com/api/embeddings")

# ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
# ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ë° ì´ˆê¸°í™”
vectorstore = FAISS.load_local(
    folder_path="./vectorstore",
    index_name="faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True)

# ì‚¬ì´ë“œë°” ë²„íŠ¼ ìƒì„±
with st.sidebar:
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["hwp","json"])

    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ì´ˆê¸°í™”")


# ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        # st.chat_message(chat_message.role).write(chat_message.content)
        with st.chat_message(chat_message.role):
            st.markdown(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì„¸ì§€ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ì¤‘ë³µ íŒŒì¼ì„ ë¹„êµí•˜ê¸° ìœ„í•´
def check_duplicate(file_hash):
    existing_hashes = {
        doc.metadata.get("file_hash")
        for doc in vectorstore.docstore._dict.values()
        if doc.metadata.get("file_hash") is not None
    }
    return file_hash in existing_hashes


# íŒŒì¼ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ:
def embed_file(file):

    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
    file_content = file.read()
    file_hash = md5(file_content).hexdigest()

    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ë§Œì•½ ê¸°ì¡´ì— ìˆë˜ ë°ì´í„°ë¼ë©´ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰
    if check_duplicate(file_hash):
        warning_msg.warning("âš ï¸ì´ë¯¸ ì—…ë¡œë“œëœ íŒŒì¼ì…ë‹ˆë‹¤!")
        retriever = vectorstore.as_retriever()
        time.sleep(3)
        warning_msg.empty()  # ê²½ê³  ë©”ì‹œì§€ ì œê±°
        return retriever # ì¤‘ë³µì´ë©´ ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    ext = os.path.splitext(file.name)[1].lower()

    # ë¬¸ì„œ ë¡œë“œ(Load Documents)
    try:
        if ext == ".hwp":
            loader = HWPLoader(file_path)
            docs = loader.load()

        elif ext == ".json":
            loader = JsonLoader(file_path)
            docs = loader.load()

        else:
            raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” í™•ì¥ì")

    except Exception as e:
        warning_msg.error("í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì˜ íŒŒì¼ì…ë‹ˆë‹¤...ğŸ˜¢")
        time.sleep(3)
        warning_msg.empty()
        return retriever

    warning_msg.warning("...ğŸ“‚ë¬¸ì„œ ì²˜ë¦¬ì¤‘ì…ë‹ˆë‹¤...")
    
    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ í•´ì‹œ ì¶”ê°€
    for doc in docs:
        doc.metadata["file_hash"] = file_hash
        doc.metadata["file_name"] = file.name

    # ë¬¸ì„œ ë¶„í• (Split Documents)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)
    split_documents = text_splitter.split_documents(docs)

    # DB ìƒì„±(Create DB) ë° ì €ì¥
    vectorstore.add_documents(split_documents, embeddings=embeddings)
    vectorstore.save_local(folder_path="./vectorstore", index_name="faiss_index")

    warning_msg.empty()
    # ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    retriever = vectorstore.as_retriever()

    return retriever

# ì²´ì¸ ìƒì„±
def create_chain(retriever, model_name="EXAONE-3.5"):
    
    # í”„ë¡¬í”„íŠ¸
    prompt = load_prompt("prompt.yaml", encoding="utf-8")
    
    # ì–¸ì–´ëª¨ë¸(LLM)
    if model_name == "EXAONE-3.5":
        llm = ChatOllama(model="EXAONE3.5-Q8:latest", temperature=0)
    
    # ì²´ì¸ ìƒì„±
    chain = (
        #{"context": retriever | format_docs, "question": RunnablePassthrough()}
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain


# íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆì„ë•Œ:
if uploaded_file:
    # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„± (ì‘ì—…ì‹œê°„ ì˜¤ë˜ê±¸ë¦´ ì˜ˆì •)
    retriever = embed_file(uploaded_file)
    chain = create_chain(retriever, model_name="EXAONE-3.5") # selected_model
    st.session_state["chain"] = chain
else:  # ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì§€ ì•Šì€ ìƒíƒœë¡œ ì…ë ¥ ì²˜ë¦¬ ; ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ëœ ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ìƒì„±
    retriever = vectorstore.as_retriever()
    chain = create_chain(retriever, model_name="EXAONE-3.5")
    st.session_state["chain"] = chain

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´
if clear_btn:
    st.session_state["messages"] = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    st.session_state["chain"] = None

# í˜¸ì¶œ
print_messages()

## ì‚¬ìš©ìì˜ ì…ë ¥ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

if user_input:  # ì‚¬ìš©ìì˜ ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´, (prompt ë³€ìˆ˜ì— ì…ë ¥ì´ ë‹´ê¹€)
    docs = retriever.invoke(user_input)  # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
    print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜:", len(docs))
    for i, doc in enumerate(docs):
        print(f"[{i}] {doc.metadata.get('file_name')} - {doc.page_content[:100]}...")

    # chain ìƒì„±
    chain = st.session_state["chain"]

    # ì‚¬ìš©ìì˜ ì…ë ¥
    # st.chat_message("user").write(user_input)
    with st.chat_message("user"):
        st.markdown(user_input)

    response = chain.stream(
        user_input
    )  # RunnablePassthroughë¡œ ë°›ì•„ì˜¤ë¯€ë¡œ ê°’ë§Œ ë„£ì–´ì£¼ë©´ë¨

    with st.chat_message("assistant"):
        # ë¹ˆ ì»¨í…Œì´ë„ˆ ë§Œë“¤ê¸° -> ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥(í† í°ë³„)
        container = st.empty()
        ai_answer = ""  # ë¹ˆ ë¬¸ìì—´ì— ì´ì–´ë¶™ì´ê¸°
        for token in response:
            ai_answer += token
            container.markdown(ai_answer) # markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥

    add_message("user", user_input)
    add_message("assistant", ai_answer)
