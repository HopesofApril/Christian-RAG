import os
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.docstore.in_memory import InMemoryDocstore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from data_loader import HWPLoader
import hashlib

# === ì„¤ì • ===
EMBEDDING_MODEL = "nomic-embed-text"
VECTORSTORE_DIR = "./vectorstore"
INDEX_NAME = "faiss_index"

# === ì„ë² ë”© ëª¨ë¸ ===
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)


def build_vectorstore_from_dir_optimized(document_dir: str):
    """HWPLoaderì—ì„œ ë°”ë¡œ ë¶„í• í•˜ì—¬ ë¶ˆí•„ìš”í•œ chunk ìƒì„± ë°©ì§€"""
    # === ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ===
    if not os.path.exists(f"{VECTORSTORE_DIR}/{INDEX_NAME}.faiss"):
        if not os.path.exists(VECTORSTORE_DIR):
            os.mkdir(VECTORSTORE_DIR)
        dim = len(embeddings.embed_query("hello world"))
        vectorstore = FAISS(
            embedding_function=embeddings,
            index=faiss.IndexFlatL2(dim),
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
        )
    else:
        vectorstore = FAISS.load_local(
            folder_path=VECTORSTORE_DIR,
            index_name=INDEX_NAME,
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )

    # === ë¬¸ì„œ ì²˜ë¦¬ ===
    for filename in os.listdir(document_dir):
        if not filename.lower().endswith(".hwp"):
            continue

        print(f"... ğŸ“„ {filename} ì§„í–‰ì¤‘ ...")
        file_path = os.path.join(document_dir, filename)
        file_hash = hashlib.md5(open(file_path, 'rb').read()).hexdigest()

        try:
            loader = HWPLoader(file_path)
            # HWPLoaderì—ì„œ ë°”ë¡œ ë¬¸ë‹¨ ë‹¨ìœ„ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ëŠ”ë‹¤ê³  ê°€ì •
            docs = loader.load(split_by_paragraph=True)  # <-- ì—¬ê¸°ì„œ ë¶„í• 
        except Exception as e:
            print(f"âš ï¸ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ â†’ ê±´ë„ˆëœë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
            continue

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for doc in docs:
            doc.metadata["file_hash"] = file_hash
            doc.metadata["file_name"] = filename

        # í›„ì²˜ë¦¬ ë¶„í•  (ê¸¸ì´ê°€ ë„ˆë¬´ ê¸´ ë¬¸ë‹¨ë§Œ ìë¥´ê¸°)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=3000,
            chunk_overlap=20,
            separators=["\n\n", "\n", " "]
        )
        split_docs = []
        for doc in docs:
            if len(doc.page_content) > 3000:
                split_docs.extend(text_splitter.split_documents([doc]))
            else:
                split_docs.append(doc)

        # ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
        vectorstore.add_documents(split_docs, embeddings=embeddings)
        print(f"ğŸ“„ {filename} ë²¡í„°í™” ë° ì €ì¥ ì™„ë£Œ! (ì´ {len(split_docs)} chunks)")

    # ìµœì¢… ì €ì¥
    vectorstore.save_local(folder_path=VECTORSTORE_DIR, index_name=INDEX_NAME)
    print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {VECTORSTORE_DIR}/{INDEX_NAME}.faiss")


# === í´ë” ì§€ì • í›„ ì‹¤í–‰ ===
if __name__ == "__main__":
    document_dir = "./2007ë…„"
    build_vectorstore_from_dir_optimized(document_dir)
